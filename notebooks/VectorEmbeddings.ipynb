{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff708c2",
   "metadata": {},
   "source": [
    "# Vector Embeddings with OpenAI - Beginner's Guide\n",
    "\n",
    "## What are Vector Embeddings?\n",
    "Think of embeddings as a way to convert words and sentences into numbers that computers can understand. Just like how we might describe a movie with ratings (comedy: 8/10, action: 3/10), embeddings describe text with many numbers.\n",
    "\n",
    "## Why do we need them?\n",
    "- **Find similar content**: Discover documents that talk about similar topics\n",
    "- **Smart search**: Search by meaning, not just matching exact words\n",
    "- **Organize information**: Group related content automatically\n",
    "\n",
    "## What you'll learn in this tutorial:\n",
    "1. How to convert text into numbers (embeddings)\n",
    "2. How to store these numbers efficiently\n",
    "3. How to find similar documents\n",
    "4. How to build a simple Q&A system that can answer questions from your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabb767",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, we need to import the tools we'll use. Think of this like getting all your ingredients ready before cooking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8c293a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install langchain langchain-openai langchain-community\n",
    "# !pip install chromadb faiss-cpu\n",
    "# !pip install python-dotenv\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a98d84",
   "metadata": {},
   "source": [
    "## 1. Creating Your First Embedding\n",
    "\n",
    "**What's happening here?**\n",
    "We're going to take a sentence and convert it into a list of numbers. Each sentence gets its own unique set of numbers, kind of like a fingerprint!\n",
    "\n",
    "**Why 3072 numbers?**\n",
    "The AI model we're using describes each sentence with 3072 different measurements. More numbers = more detailed description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace18005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: LangChain is a framework for developing applications powered by language models.\n",
      "Embedding dimension: 3072\n",
      "First 10 values: [-0.04118821769952774, -0.011433425359427929, -0.03338002413511276, 0.0036600905004888773, -0.03455125540494919, -0.01826559379696846, -0.011649545282125473, 0.02858356386423111, -0.022155746817588806, 0.009781155735254288]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set up the embedding generator\n",
    "# This connects to Azure OpenAI to convert text into numbers\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")# The AI model that creates embeddings\n",
    ")\n",
    "\n",
    "# Step 2: Convert one sentence into numbers\n",
    "text = \"LangChain is a framework for developing applications powered by language models.\"\n",
    "embedding_vector = embeddings.embed_query(text)\n",
    "\n",
    "# Step 3: See the results\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding dimension: {len(embedding_vector)}\")  # How many numbers describe this sentence\n",
    "print(f\"First 10 values: {embedding_vector[:10]}\")  # Just showing the first 10 numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c539f9c",
   "metadata": {},
   "source": [
    "## 2. Converting Multiple Sentences\n",
    "\n",
    "Now let's convert several sentences at once. This is useful when you have many documents to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e0cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5\n",
      "Embedding dimension: 3072\n",
      "\n",
      "First document: Python is a high-level programming language known for its simplicity.\n",
      "Its embedding (first 5 values): [-0.031239548698067665, -0.02715788409113884, -0.012222188524901867, 0.02540208399295807, -0.0032550697214901447]\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents\n",
    "texts = [\n",
    "    \"Python is a high-level programming language known for its simplicity.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"LangChain provides tools for building LLM applications.\",\n",
    "    \"Vector databases store embeddings for efficient similarity search.\",\n",
    "    \"Azure OpenAI Service provides access to powerful language models.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for multiple documents\n",
    "document_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "print(f\"Number of documents: {len(document_embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(document_embeddings[0])}\")\n",
    "print(f\"\\nFirst document: {texts[0]}\")\n",
    "print(f\"Its embedding (first 5 values): {document_embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f81b50",
   "metadata": {},
   "source": [
    "## 3. Storing Embeddings - Meet FAISS\n",
    "\n",
    "**What is FAISS?**\n",
    "FAISS is like a super-organized filing cabinet for your embeddings. It stores them in a way that makes finding similar documents really fast.\n",
    "\n",
    "**Why use it?**\n",
    "Instead of comparing your search with every single document (which is slow), FAISS finds similar documents almost instantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29428ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created successfully!\n",
      "Number of documents in store: 7\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a list of sentences (our mini-document collection)\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning algorithms learn patterns from data.\",\n",
    "    \"Python is widely used in data science and AI.\",\n",
    "    \"Natural language processing helps computers understand human language.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Cloud computing provides on-demand computing resources.\",\n",
    "    \"Azure is Microsoft's cloud platform for building and deploying applications.\"\n",
    "]\n",
    "\n",
    "# Step 2: Convert all sentences to embeddings and store them in FAISS\n",
    "# FAISS will organize them for fast searching\n",
    "vectorstore_faiss = FAISS.from_texts(documents, embeddings)\n",
    "\n",
    "# Step 3: Confirm it worked\n",
    "print(\"FAISS vector store created successfully!\")\n",
    "print(f\"Number of documents in store: {vectorstore_faiss.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cb66b",
   "metadata": {},
   "source": [
    "## 4. Finding Similar Documents\n",
    "\n",
    "**The Magic Moment!**\n",
    "Now we can search for documents by meaning, not just exact words. \n",
    "\n",
    "For example: If you search \"AI learning\", it will find documents about \"machine learning\" even though the exact words don't match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f22ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about artificial intelligence and learning from data\n",
      "\n",
      "Top 3 most similar documents:\n",
      "\n",
      "1. Machine learning algorithms learn patterns from data.\n",
      "\n",
      "2. Python is widely used in data science and AI.\n",
      "\n",
      "3. Deep learning uses neural networks with multiple layers.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Write your search question\n",
    "query = \"Tell me about artificial intelligence and learning from data\"\n",
    "\n",
    "# Step 2: Find the 3 most similar documents\n",
    "# k=3 means \"give me the top 3 results\"\n",
    "results = vectorstore_faiss.similarity_search(query, k=3)\n",
    "\n",
    "# Step 3: Show the results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 most similar documents:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d6ed5",
   "metadata": {},
   "source": [
    "## 5. Understanding Similarity Scores\n",
    "\n",
    "**What are these scores?**\n",
    "Lower score = More similar (closer match)\n",
    "Higher score = Less similar (further apart)\n",
    "\n",
    "Think of it like distance: The closer two things are, the smaller the distance between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf634a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about artificial intelligence and learning from data\n",
      "\n",
      "Results with similarity scores:\n",
      "\n",
      "1. Score: 0.9004\n",
      "   Document: Machine learning algorithms learn patterns from data.\n",
      "\n",
      "2. Score: 1.1342\n",
      "   Document: Python is widely used in data science and AI.\n",
      "\n",
      "3. Score: 1.2108\n",
      "   Document: Deep learning uses neural networks with multiple layers.\n"
     ]
    }
   ],
   "source": [
    "# Similarity search with scores (lower score = more similar)\n",
    "results_with_scores = vectorstore_faiss.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Results with similarity scores:\")\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Document: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf80e2",
   "metadata": {},
   "source": [
    "## 6. Another Storage Option - Chroma\n",
    "\n",
    "**What's different about Chroma?**\n",
    "Chroma is another way to store embeddings, but it lets you add extra information (metadata) to each document.\n",
    "\n",
    "**Example**: You can tag documents as \"beginner\", \"intermediate\", or \"advanced\" and then search only within that category!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2822a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma vector store created successfully!\n",
      "Number of documents: 4\n"
     ]
    }
   ],
   "source": [
    "# Create Chroma vector store with metadata\n",
    "docs_with_metadata = [\n",
    "    Document(page_content=\"Python is great for data science.\", metadata={\"topic\": \"programming\", \"level\": \"beginner\"}),\n",
    "    Document(page_content=\"Machine learning requires understanding of statistics.\", metadata={\"topic\": \"ML\", \"level\": \"intermediate\"}),\n",
    "    Document(page_content=\"Deep neural networks power modern AI.\", metadata={\"topic\": \"AI\", \"level\": \"advanced\"}),\n",
    "    Document(page_content=\"LangChain simplifies LLM application development.\", metadata={\"topic\": \"tools\", \"level\": \"intermediate\"}),\n",
    "]\n",
    "\n",
    "# Create Chroma vector store\n",
    "vectorstore_chroma = Chroma.from_documents(\n",
    "    documents=docs_with_metadata,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"langchain_tutorial\"\n",
    ")\n",
    "\n",
    "print(\"Chroma vector store created successfully!\")\n",
    "print(f\"Number of documents: {vectorstore_chroma._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f82805",
   "metadata": {},
   "source": [
    "## 7. Filtering Your Search\n",
    "\n",
    "**Why is this useful?**\n",
    "Imagine you have 1000 documents. You can search only within documents tagged as \"intermediate level\" instead of searching through everything!\n",
    "\n",
    "It's like filtering products by price range when shopping online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cab60a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: artificial intelligence\n",
      "Filtered results (intermediate level only):\n",
      "\n",
      "1. Machine learning requires understanding of statistics.\n",
      "   Metadata: {'level': 'intermediate', 'topic': 'ML'}\n",
      "\n",
      "2. LangChain simplifies LLM application development.\n",
      "   Metadata: {'topic': 'tools', 'level': 'intermediate'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search with metadata filtering\n",
    "query = \"artificial intelligence\"\n",
    "\n",
    "# Filter for intermediate level documents only\n",
    "results = vectorstore_chroma.similarity_search(\n",
    "    query,\n",
    "    k=2,\n",
    "    filter={\"level\": \"intermediate\"}\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Filtered results (intermediate level only):\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "    print(f\"   Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efbf98",
   "metadata": {},
   "source": [
    "## 8. Breaking Long Documents into Pieces\n",
    "\n",
    "**The Problem**: Documents can be very long, but embeddings work best with shorter pieces of text.\n",
    "\n",
    "**The Solution**: Split long documents into smaller \"chunks\" that overlap a little bit.\n",
    "\n",
    "**Why overlap?**: So we don't lose important information that might be split between chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e8f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 865 characters\n",
      "Number of chunks: 7\n",
      "\n",
      "Chunks:\n",
      "\n",
      "Chunk 1 (171 chars):\n",
      "LangChain is a framework for developing applications powered by language models. \n",
      "It enables applications that are context-aware and can reason about the provided context.\n",
      "\n",
      "Chunk 2 (173 chars):\n",
      "The framework consists of several key components. First, there are LLM wrappers that \n",
      "provide a common interface for different language models. Second, prompt templates help\n",
      "\n",
      "Chunk 3 (48 chars):\n",
      "structure inputs to language models effectively.\n",
      "\n",
      "Chunk 4 (174 chars):\n",
      "LangChain also provides chains, which are sequences of calls to LLMs or other utilities. \n",
      "Agents use LLMs to decide which actions to take and in what order. Memory components\n",
      "\n",
      "Chunk 5 (58 chars):\n",
      "allow chains and agents to remember previous interactions.\n",
      "\n",
      "Chunk 6 (173 chars):\n",
      "Vector stores and embeddings enable semantic search capabilities. This is crucial for \n",
      "building retrieval augmented generation (RAG) systems that can access and use external\n",
      "\n",
      "Chunk 7 (54 chars):\n",
      "knowledge sources to answer questions more accurately.\n"
     ]
    }
   ],
   "source": [
    "# Sample long document\n",
    "long_text = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "It enables applications that are context-aware and can reason about the provided context.\n",
    "\n",
    "The framework consists of several key components. First, there are LLM wrappers that \n",
    "provide a common interface for different language models. Second, prompt templates help \n",
    "structure inputs to language models effectively.\n",
    "\n",
    "LangChain also provides chains, which are sequences of calls to LLMs or other utilities. \n",
    "Agents use LLMs to decide which actions to take and in what order. Memory components \n",
    "allow chains and agents to remember previous interactions.\n",
    "\n",
    "Vector stores and embeddings enable semantic search capabilities. This is crucial for \n",
    "building retrieval augmented generation (RAG) systems that can access and use external \n",
    "knowledge sources to answer questions more accurately.\n",
    "\"\"\"\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "chunks = text_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Original text length: {len(long_text)} characters\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(\"\\nChunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66dd9b",
   "metadata": {},
   "source": [
    "## 9. Building a Q&A System (RAG)\n",
    "\n",
    "**What is RAG?**\n",
    "RAG stands for \"Retrieval Augmented Generation\". That's a fancy way of saying:\n",
    "1. **Retrieval**: Find relevant documents from your collection\n",
    "2. **Generation**: Use AI to write an answer based on those documents\n",
    "\n",
    "**Think of it like**: \n",
    "- You ask a question\n",
    "- The system finds relevant information in your documents\n",
    "- AI reads that information and gives you an answer\n",
    "\n",
    "Let's create our knowledge base first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a77b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base created successfully!\n",
      "Documents in knowledge base: 7\n"
     ]
    }
   ],
   "source": [
    "# Create knowledge base\n",
    "knowledge_base = [\n",
    "    \"LangChain was created by Harrison Chase in 2022.\",\n",
    "    \"LangChain supports multiple LLM providers including OpenAI, Azure, and Anthropic.\",\n",
    "    \"The main components of LangChain are: Models, Prompts, Chains, Agents, and Memory.\",\n",
    "    \"Vector stores in LangChain help with semantic search and retrieval.\",\n",
    "    \"LangChain Expression Language (LCEL) allows you to build chains using the pipe operator.\",\n",
    "    \"Agents in LangChain can use tools to interact with external systems.\",\n",
    "    \"Memory components allow chatbots to remember conversation history.\",\n",
    "]\n",
    "\n",
    "# Create vector store from knowledge base\n",
    "knowledge_vectorstore = FAISS.from_texts(knowledge_base, embeddings)\n",
    "\n",
    "print(\"Knowledge base created successfully!\")\n",
    "print(f\"Documents in knowledge base: {len(knowledge_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dde83b",
   "metadata": {},
   "source": [
    "## 10. Creating the Q&A Chain\n",
    "\n",
    "**What's a \"chain\"?**\n",
    "A chain is a series of steps that happen in order:\n",
    "1. User asks a question\n",
    "2. System finds 3 most relevant documents\n",
    "3. AI reads those documents\n",
    "4. AI writes an answer based only on those documents\n",
    "\n",
    "**Why \"only on those documents\"?**\n",
    "This ensures the AI doesn't make up information - it can only use what's in your documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d34450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who created LangChain and when?\n",
      "\n",
      "Answer: LangChain was created by Harrison Chase in 2022.\n",
      "\n",
      "Source documents used:\n",
      "1. LangChain was created by Harrison Chase in 2022.\n",
      "2. The main components of LangChain are: Models, Prompts, Chains, Agents, and Memory.\n",
      "3. LangChain supports multiple LLM providers including OpenAI, Azure, and Anthropic.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Connect to the AI that will write answers\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    temperature=0  # 0 = more focused answers, 1 = more creative answers\n",
    ")\n",
    "\n",
    "# Step 2: Create a template for how to ask questions\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Step 3: Set up the document finder\n",
    "# It will search for the top 3 most relevant documents\n",
    "retriever = knowledge_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Step 4: Create a helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Combine multiple documents into one text block\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Step 5: Build the complete Q&A chain\n",
    "# This connects: search â†’ format â†’ ask AI â†’ get answer\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 6: Try it out with a question!\n",
    "question = \"Who created LangChain and when?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "# Step 7: Show which documents were used to answer\n",
    "source_docs = retriever.invoke(question)\n",
    "print(f\"\\nSource documents used:\")\n",
    "for i, doc in enumerate(source_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cbfbc5",
   "metadata": {},
   "source": [
    "## 11. Testing with More Questions\n",
    "\n",
    "Let's try several different questions to see how well our Q&A system works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d634dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the main components of LangChain?\n",
      "A: The main components of LangChain are Models, Prompts, Chains, Agents, and Memory.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: What is LCEL?\n",
      "A: LangChain Expression Language (LCEL) is a feature that allows you to build chains using the pipe operator.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Q: What LLM providers does LangChain support?\n",
      "A: LangChain supports multiple LLM providers including OpenAI, Azure, and Anthropic.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions\n",
    "questions = [\n",
    "    \"What are the main components of LangChain?\",\n",
    "    \"What is LCEL?\",\n",
    "    \"What LLM providers does LangChain support?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5c1ba",
   "metadata": {},
   "source": [
    "## 12. Advanced: Getting Diverse Results (MMR)\n",
    "\n",
    "**The Problem**: Sometimes similarity search returns very similar documents (almost duplicates).\n",
    "\n",
    "**The Solution**: MMR (Maximal Marginal Relevance) finds documents that are:\n",
    "- Relevant to your question\n",
    "- Different from each other\n",
    "\n",
    "**Example**: Instead of getting 3 documents all saying \"Python is popular\", you might get:\n",
    "1. Python is popular\n",
    "2. Python has many libraries\n",
    "3. Python is used in AI\n",
    "\n",
    "See the difference below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e42fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Standard Similarity Search ===\n",
      "1. LangChain supports multiple LLM providers including OpenAI, Azure, and Anthropic.\n",
      "2. The main components of LangChain are: Models, Prompts, Chains, Agents, and Memory.\n",
      "3. LangChain was created by Harrison Chase in 2022.\n",
      "\n",
      "=== MMR Search (more diverse results) ===\n",
      "1. LangChain supports multiple LLM providers including OpenAI, Azure, and Anthropic.\n",
      "2. LangChain Expression Language (LCEL) allows you to build chains using the pipe operator.\n",
      "3. The main components of LangChain are: Models, Prompts, Chains, Agents, and Memory.\n"
     ]
    }
   ],
   "source": [
    "# Create retriever with MMR\n",
    "retriever_mmr = knowledge_vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3, \"fetch_k\": 5}\n",
    ")\n",
    "\n",
    "# Compare standard similarity search vs MMR\n",
    "query = \"LangChain components and features\"\n",
    "\n",
    "print(\"=== Standard Similarity Search ===\")\n",
    "standard_results = knowledge_vectorstore.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(standard_results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "\n",
    "print(\"\\n=== MMR Search (more diverse results) ===\")\n",
    "mmr_results = retriever_mmr.invoke(query)\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa48a42",
   "metadata": {},
   "source": [
    "## 13. Saving Your Work\n",
    "\n",
    "**Why save?**\n",
    "Creating embeddings takes time and costs money (API calls). By saving them, you can reuse them later without recreating everything!\n",
    "\n",
    "**When to use this?**\n",
    "- You have a large document collection\n",
    "- You want to use the same documents multiple times\n",
    "- You want to share your vector store with others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41296ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to 'faiss_index' directory\n",
      "FAISS index loaded successfully!\n",
      "\n",
      "Test query: cloud computing\n",
      "1. Cloud computing provides on-demand computing resources.\n",
      "2. Azure is Microsoft's cloud platform for building and deploying applications.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Save the vector store to your computer\n",
    "vectorstore_faiss.save_local(\"faiss_index\")\n",
    "print(\"FAISS index saved to 'faiss_index' directory\")\n",
    "\n",
    "# Step 2: Load it back (useful when you restart your program)\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\", \n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for security\n",
    ")\n",
    "print(\"FAISS index loaded successfully!\")\n",
    "\n",
    "# Step 3: Test that it still works\n",
    "test_query = \"cloud computing\"\n",
    "results = loaded_vectorstore.similarity_search(test_query, k=2)\n",
    "print(f\"\\nTest query: {test_query}\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f536b5",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations! You've Completed the Tutorial!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "1. **Embeddings** = Converting text to numbers so computers can understand meaning\n",
    "2. **Vector Stores** (FAISS & Chroma) = Smart storage for fast searching\n",
    "3. **Similarity Search** = Finding documents by meaning, not just keywords\n",
    "4. **Metadata** = Adding tags to filter your searches\n",
    "5. **Text Splitting** = Breaking long documents into digestible pieces\n",
    "6. **RAG System** = Building a Q&A bot that answers from your documents\n",
    "7. **MMR** = Getting diverse, non-repetitive results\n",
    "8. **Persistence** = Saving your work for later\n",
    "\n",
    "### ðŸš€ What's Next?\n",
    "\n",
    "**Easy Next Steps:**\n",
    "- Try with your own text documents\n",
    "- Experiment with different chunk sizes (try 100, 300, 500)\n",
    "- Add more documents to your knowledge base\n",
    "\n",
    "**Intermediate Challenges:**\n",
    "- Build a Q&A system for a PDF document\n",
    "- Create a chatbot that remembers conversation history\n",
    "- Try different embedding models\n",
    "\n",
    "**Advanced Ideas:**\n",
    "- Build a search engine for your personal notes\n",
    "- Create a recommendation system\n",
    "- Combine multiple data sources\n",
    "\n",
    "### ðŸ’¡ Key Takeaway\n",
    "You now know how to build AI applications that can understand and search through text by meaning, not just keywords. This is the foundation of modern AI search systems!\n",
    "\n",
    "**Questions?** Review the earlier sections or try running the code again with different examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
