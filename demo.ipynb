{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f979f1f2",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals Tutorial\n",
    "\n",
    "Welcome to this comprehensive guide on **LangChain** - a powerful framework for building applications with Large Language Models (LLMs).\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "LangChain is a framework designed to simplify the creation of applications using large language models. It provides:\n",
    "- **Modular components** for working with LLMs\n",
    "- **Chains** to combine multiple components\n",
    "- **Memory** to maintain conversation context\n",
    "- **Agents** for autonomous decision-making\n",
    "- **Tools** for external integrations\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Setting up LangChain\n",
    "2. Working with LLMs and Chat Models\n",
    "3. Prompt Templates\n",
    "4. Chains\n",
    "5. Memory Management\n",
    "6. Agents and Tools\n",
    "7. Practical Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26136f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "# !pip install langchain langchain-openai langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fc964",
   "metadata": {},
   "source": [
    "**Note:** Create a `.env` file in your project root with your API keys:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "# or for Azure OpenAI\n",
    "AZURE_OPENAI_API_KEY=your_azure_key\n",
    "AZURE_OPENAI_ENDPOINT=your_azure_endpoint\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55539a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain, a framework designed for building applications with language models, revolves around three main concepts:\n",
      "\n",
      "1. **Chains**: Chains involve sequences of operations or calls where language models are used as components. Instead of directly interacting with a language model, you can create a chain of prompts, actions, and logic that combine to accomplish complex tasks, such as retrieving information, processing data, or interacting with APIs.\n",
      "\n",
      "2. **Agents**: Agents are decision-making entities powered by language models. They have the capability to dynamically determine which actions to take or tools to use based on the input and the context. Agents are particularly useful for tasks requiring flexibility and adaptability, such as solving problems, answering questions, or interacting with external systems.\n",
      "\n",
      "3. **Memory**: Memory provides a way for chains or agents to retain context or state across interactions. This enables applications to maintain conversational continuity or remember previous interactions, making them ideal for tasks like chatbots or multi-step workflows.\n",
      "\n",
      "These concepts work together to enable developers to create sophisticated applications leveraging the power of language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Initialize Azure OpenAI Chat Model\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://lnguyentuan-azure-openai.openai.azure.com/\"),\n",
    "    api_key=os.getenv(\"KEY\"),\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Simple invocation\n",
    "response = llm.invoke(\"What are the three main concepts in LangChain?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6693a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Prompt:\n",
      "Explain machine learning in simple terms for a beginner.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Chat Prompt:\n",
      "SystemMessage: You are a helpful AI assistant that explains technical concepts clearly.\n",
      "HumanMessage: Explain neural networks to a high school student.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Simple Prompt Template\n",
    "simple_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms for a beginner.\"\n",
    ")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = simple_template.format(topic=\"machine learning\")\n",
    "print(\"Simple Prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Chat Prompt Template (for chat models)\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant that explains technical concepts clearly.\"),\n",
    "    (\"user\", \"Explain {topic} to a {audience}.\")\n",
    "])\n",
    "\n",
    "# Format the chat prompt\n",
    "formatted_chat = chat_template.format_messages(\n",
    "    topic=\"neural networks\",\n",
    "    audience=\"high school student\"\n",
    ")\n",
    "print(\"Chat Prompt:\")\n",
    "for msg in formatted_chat:\n",
    "    print(f\"{msg.__class__.__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b28a4",
   "metadata": {},
   "source": [
    "## 4. Chains - Combining Components\n",
    "\n",
    "Chains allow you to combine multiple components (prompts, LLMs, and other chains) into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83185dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Response:\n",
      "Sure! Neural networks might sound complicated, but letâ€™s break it down into simple ideas. Imagine youâ€™re trying to teach a computer how to recognize something, like whether a photo has a dog or a cat in it. Neural networks are a tool that helps the computer learn how to figure this out.\n",
      "\n",
      "### 1. Inspired by the Brain\n",
      "Neural networks are loosely inspired by the human brain. Your brain has billions of tiny cells called neurons, which work together to help you think, learn, and make decisions. Neural networks try to mimic how these neurons work, but they do it in a much simpler way.\n",
      "\n",
      "### 2. The Building Blocks: Layers and Nodes\n",
      "Think of a neural network like a big team of workers organized into layers. Each worker is a \"node\" (or artificial neuron), and each layer has a group of nodes.\n",
      "\n",
      "- **Input Layer**: This is the first layer of the network. It takes in the raw data, like the pixels in a photo of a dog or a cat.\n",
      "- **Hidden Layers**: These are the layers in the middle. They do the hard work of figuring out patterns in the data, like noticing that dogs usually have floppy ears and cats usually have pointy ears.\n",
      "- **Output Layer**: This is the final layer. It gives the answer, like whether the photo has a dog or a cat.\n",
      "\n",
      "### 3. Connections and Weights\n",
      "The nodes in one layer are connected to nodes in the next layer. Think of these connections like roads. Each connection has a \"weight,\" which is like a speed limit on the road. The weights control how much influence one node has on the next. During training, the neural network adjusts these weights to learn better answers.\n",
      "\n",
      "### 4. How It Learns: Training\n",
      "To train a neural network, you show it examples. For instance, you give it lots of photos of dogs and cats, along with the correct answer (dog or cat). At first, the network might guess randomly, but then it compares its guess to the correct answer and sees how far off it was. It uses this information to adjust the weights on its connections, so it gets better at guessing next time. This process is repeated thousands or even millions of times until the network becomes really good at recognizing dogs and cats.\n",
      "\n",
      "### 5. Why Is It Useful?\n",
      "Neural networks are super useful because they can learn to recognize patterns in data. Theyâ€™re used for all kinds of things, like:\n",
      "- Recognizing faces in photos.\n",
      "- Suggesting songs or movies you might like.\n",
      "- Helping cars drive themselves.\n",
      "- Detecting diseases in medical images.\n",
      "\n",
      "### 6. A Fun Analogy\n",
      "Imagine youâ€™re baking cookies, and you want them to taste just right. At first, you might guess how much sugar and chocolate chips to use. You bake the cookies, taste them, and realize theyâ€™re too sweet or not chocolatey enough. So you adjust the amounts and try again. After a few tries, your recipe is perfect! Neural networks work in a similar wayâ€”they keep adjusting their \"recipe\" (the weights) until they give good answers.\n",
      "\n",
      "### Summary\n",
      "A neural network is like a computerâ€™s way of learning from examples. Itâ€™s made up of layers of nodes connected by weights, and it gets better at solving problems through practice and adjustments. Itâ€™s not as smart as a human brain, but itâ€™s really good at specific tasks, like recognizing patterns or making predictions.\n",
      "\n",
      "Does that make sense? Let me know if you want me to explain anything more!\n"
     ]
    }
   ],
   "source": [
    "# Use the prompt template with LLM\n",
    "response = llm.invoke(formatted_chat)\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are three interesting hypothetical facts about the topic \"From Algorithms to Emotions: Exploring the Possibility of AI with Human-Like Empathy\":\n",
      "\n",
      "1. **AI Empathy Modeling**: Research in AI empathy explores how machines might simulate the emotional intelligence needed to understand and respond to human emotions authentically. Techniques such as sentiment analysis, natural language processing, and affective computing are being integrated to recognize emotional cues in facial expressions, voice tones, and written language. While AI systems can mimic empathy, creating true emotional experiences remains a major ethical and technical challenge.\n",
      "\n",
      "2. **Potential Applications**: AI equipped with human-like empathy could revolutionize industries like mental health care, customer service, and education. For example, empathetic AI could serve as virtual therapists capable of offering personalized emotional support or as tutors who adapt their teaching methods based on the stress or frustration levels of students. These systems could help improve human well-being and communication on an unprecedented scale.\n",
      "\n",
      "3. **Ethical Concerns**: The development of empathetic AI poses significant ethical dilemmas. Critics fear that machines capable of simulating empathy may manipulate human emotions for profit, such as in marketing, or be used in malicious ways, like influencing public opinion. Additionally, questions arise about whether machines can truly \"feel\" empathy or if it is merely an algorithmic simulation, raising philosophical debates about consciousness and the nature of emotions.\n"
     ]
    }
   ],
   "source": [
    "# Sequential Chain Example - Multiple steps\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "topic_prompt = ChatPromptTemplate.from_template(\"Suggest a creative topic about {subject}\")\n",
    "topic_chain = topic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Write about that topic\n",
    "writing_prompt = ChatPromptTemplate.from_template(\"Write 3 interesting facts about: {topic}\")\n",
    "writing_chain = writing_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine them\n",
    "full_chain = {\"topic\": topic_chain} | writing_chain\n",
    "\n",
    "result = full_chain.invoke({\"subject\": \"artificial intelligence\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question:\n",
      "Assistant: Hi Alex! That's awesome to hear that you love programming! What kind of programming do you enjoy most, or what languages are your favorite to use?\n",
      "\n",
      "\n",
      "Second question:\n",
      "Assistant: Your name is Alex! ðŸ˜Š\n",
      "\n",
      "\n",
      "Third question:\n",
      "Assistant: You love programming! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create message history\n",
    "message_history = ChatMessageHistory()\n",
    "\n",
    "# Create a prompt with memory\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chain with memory\n",
    "chain = prompt | llm\n",
    "\n",
    "# Wrapper to handle history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "print(\"First question:\")\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi! My name is Alex and I love programming.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(f\"Assistant: {response1.content}\\n\")\n",
    "\n",
    "# Second interaction - the model should remember\n",
    "print(\"\\nSecond question:\")\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(f\"Assistant: {response2.content}\\n\")\n",
    "\n",
    "# Third interaction\n",
    "print(\"\\nThird question:\")\n",
    "response3 = chain_with_history.invoke(\n",
    "    {\"input\": \"What do I love?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(f\"Assistant: {response3.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d93edf",
   "metadata": {},
   "source": [
    "## 6. Agents and Tools\n",
    "\n",
    "Agents can decide which tools to use based on the user's input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5449602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current conversation history:\n",
      "Messages in history: 6\n",
      "human: Hi! My name is Alex and I love programming.\n",
      "ai: Hi Alex! That's awesome to hear that you love programming! What kind of programming do you enjoy most, or what languages are your favorite to use?\n",
      "human: What's my name?\n",
      "ai: Your name is Alex! ðŸ˜Š\n",
      "human: What do I love?\n",
      "ai: You love programming! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# View current memory\n",
    "print(\"Current conversation history:\")\n",
    "print(f\"Messages in history: {len(message_history.messages)}\")\n",
    "for msg in message_history.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "  - get_word_length: Returns the length of a word.\n",
      "  - multiply: Multiplies two numbers together.\n",
      "\n",
      "==================================================\n",
      "Tool Test 1: Finding word length\n",
      "The word 'LangChain' has 9 letters\n",
      "\n",
      "==================================================\n",
      "Tool Test 2: Multiplication\n",
      "15 Ã— 7 = 105\n",
      "\n",
      "==================================================\n",
      "\n",
      "Note: For full agent capabilities with tool calling, you would need:\n",
      "1. An LLM model that supports function/tool calling (like GPT-4)\n",
      "2. The langgraph package for advanced agent orchestration\n",
      "3. Example: from langgraph.prebuilt import create_react_agent\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define custom tools using decorator\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Create tools list\n",
    "tools = [get_word_length, multiply]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description}\")\n",
    "\n",
    "# Simple demonstration of tool usage\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Tool Test 1: Finding word length\")\n",
    "word_to_test = \"LangChain\"\n",
    "result = get_word_length.invoke(word_to_test)\n",
    "print(f\"The word '{word_to_test}' has {result} letters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Tool Test 2: Multiplication\")\n",
    "result = multiply.invoke({\"a\": 15, \"b\": 7})\n",
    "print(f\"15 Ã— 7 = {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nNote: For full agent capabilities with tool calling, you would need:\")\n",
    "print(\"1. An LLM model that supports function/tool calling (like GPT-4)\")\n",
    "print(\"2. The langgraph package for advanced agent orchestration\")\n",
    "print(\"3. Example: from langgraph.prebuilt import create_react_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b4845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON output:\n",
      "{'title': '1984', 'author': 'George Orwell', 'year': 1949, 'genre': 'Dystopian'}\n",
      "\n",
      "Title: 1984\n",
      "Author: George Orwell\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define expected output structure\n",
    "class Book(BaseModel):\n",
    "    title: str = Field(description=\"The title of the book\")\n",
    "    author: str = Field(description=\"The author of the book\")\n",
    "    year: int = Field(description=\"The year of publication\")\n",
    "    genre: str = Field(description=\"The genre of the book\")\n",
    "\n",
    "# Create parser\n",
    "parser = JsonOutputParser(pydantic_object=Book)\n",
    "\n",
    "# Create prompt with format instructions\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that outputs JSON.\"),\n",
    "    (\"user\", \"{query}\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\n",
    "    \"query\": \"Tell me about the book '1984'\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(\"Parsed JSON output:\")\n",
    "print(result)\n",
    "print(f\"\\nTitle: {result['title']}\")\n",
    "print(f\"Author: {result['author']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3740d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: What is a decorator in Python?\n",
      "============================================================\n",
      "\n",
      "Answer: A **decorator** in Python is a design pattern that allows you to modify or enhance the behavior of a function or a method without changing its actual code. It is essentially a higher-order function that takes another function as input and extends or alters its functionality.\n",
      "\n",
      "Decorators are often used for logging, authentication, modifying inputs/outputs, and other similar tasks. They are applied using the `@` syntax, which makes them easy to use and improves code readability.\n",
      "\n",
      "### How Decorators Work\n",
      "A decorator is a function that takes another function as an argument, wraps it in some logic, and returns a new function with the modified behavior. Here's the general structure:\n",
      "\n",
      "```python\n",
      "def decorator_function(original_function):\n",
      "    def wrapper_function(*args, **kwargs):\n",
      "        # Code to execute before the original function\n",
      "        print(\"Wrapper executed before the original function.\")\n",
      "        \n",
      "        # Call the original function\n",
      "        result = original_function(*args, **kwargs)\n",
      "        \n",
      "        # Code to execute after the original function\n",
      "        print(\"Wrapper executed after the original function.\")\n",
      "        \n",
      "        return result\n",
      "    return wrapper_function\n",
      "```\n",
      "\n",
      "### Applying a Decorator\n",
      "The `@decorator_name` syntax is used to apply a decorator to a function. For example:\n",
      "\n",
      "```python\n",
      "@decorator_function\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "\n",
      "say_hello()\n",
      "```\n",
      "\n",
      "This is functionally equivalent to:\n",
      "\n",
      "```python\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "\n",
      "say_hello = decorator_function(say_hello)\n",
      "say_hello()\n",
      "```\n",
      "\n",
      "### Example: Logging Decorator\n",
      "\n",
      "Hereâ€™s a practical example of a decorator that logs the execution of a function:\n",
      "\n",
      "```python\n",
      "def log_decorator(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(f\"Function '{func.__name__}' is called with arguments {args} and {kwargs}.\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(f\"Function '{func.__name__}' returned {result}.\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "@log_decorator\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "# Call the decorated function\n",
      "add(3, 5)\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Function 'add' is called with arguments (3, 5) and {}.\n",
      "Function 'add' returned 8.\n",
      "```\n",
      "\n",
      "### Key Points About Decorators\n",
      "1. **Maintainability:** Decorators help separate cross-cutting concerns (e.g., logging, validation) from the business logic of the functions.\n",
      "2. **Flexibility:** They can be applied to any callable object (functions or methods).\n",
      "3. **Reusability:** Once written, a decorator can be reused across multiple functions.\n",
      "\n",
      "Decorators are a powerful feature in Python, and understanding them can significantly improve your ability to write clean and modular code.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: Can you show me an example?\n",
      "============================================================\n",
      "\n",
      "Answer: Certainly! Let's walk through a simple example of a **decorator** in Python that enhances the functionality of a function.\n",
      "\n",
      "### Example: Timing a Function with a Decorator\n",
      "\n",
      "Let's create a decorator that measures the execution time of a function and prints it.\n",
      "\n",
      "```python\n",
      "import time\n",
      "\n",
      "# Define the decorator\n",
      "def timing_decorator(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        # Record the start time\n",
      "        start_time = time.time()\n",
      "        \n",
      "        # Call the original function\n",
      "        result = func(*args, **kwargs)\n",
      "        \n",
      "        # Record the end time\n",
      "        end_time = time.time()\n",
      "        \n",
      "        # Calculate and print the elapsed time\n",
      "        elapsed_time = end_time - start_time\n",
      "        print(f\"Function '{func.__name__}' executed in {elapsed_time:.4f} seconds.\")\n",
      "        \n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "# Apply the decorator\n",
      "@timing_decorator\n",
      "def slow_function():\n",
      "    print(\"Starting slow function...\")\n",
      "    time.sleep(2)  # Simulate a delay\n",
      "    print(\"Finished slow function!\")\n",
      "\n",
      "# Call the decorated function\n",
      "slow_function()\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Starting slow function...\n",
      "Finished slow function!\n",
      "Function 'slow_function' executed in 2.0021 seconds.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### How It Works:\n",
      "1. The `timing_decorator` function takes another function (`func`) as its input.\n",
      "2. The `wrapper` function inside the decorator:\n",
      "   - Records the start time before calling the original function.\n",
      "   - Calls the original function with any arguments (`*args` and `**kwargs`).\n",
      "   - Records the end time after the function call.\n",
      "   - Calculates and prints the time the function took to execute.\n",
      "3. The `@timing_decorator` syntax applies the `timing_decorator` to the `slow_function`.\n",
      "4. When `slow_function()` is called, it is wrapped by the `wrapper` function, which adds the timing behavior.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Decorator with Arguments\n",
      "\n",
      "You can also create decorators that accept arguments. Here's an example of a decorator that repeats a function a given number of times:\n",
      "\n",
      "```python\n",
      "def repeat_decorator(times):\n",
      "    def decorator(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            for _ in range(times):\n",
      "                func(*args, **kwargs)\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "@repeat_decorator(times=3)\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "\n",
      "# Call the decorated function\n",
      "greet(\"Alice\")\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Hello, Alice!\n",
      "Hello, Alice!\n",
      "Hello, Alice!\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Summary:\n",
      "Decorators are powerful tools that let you modify or extend the behavior of functions. By wrapping a function with a decorator, you can cleanly add functionality like:\n",
      "- Timing\n",
      "- Logging\n",
      "- Input validation\n",
      "- Repeating actions\n",
      "- Authorization checks, etc.\n",
      "\n",
      "Let me know if you'd like more examples or clarification!\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: How does this relate to LangChain?\n",
      "============================================================\n",
      "\n",
      "Answer: Great question! In the context of **LangChain**, decorators can play a significant role in enhancing or customizing the behavior of various components, such as chains, tools, agents, and memory modules. LangChain is designed to build applications powered by LLMs (like OpenAI's GPT), and decorators can help streamline processes or add extra functionality at different points in the workflow.\n",
      "\n",
      "Here are a few examples of how decorators can be relevant in LangChain:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Logging LLM Calls**\n",
      "LangChain often involves calling Language Models (LLMs), and you might want to log or monitor these calls. A decorator can help you track the inputs, outputs, and execution time of LLM invocations.\n",
      "\n",
      "#### Example:\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "import time\n",
      "\n",
      "# Define a decorator to log LLM calls\n",
      "def log_llm_calls(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(f\"Calling LLM with args: {args}, kwargs: {kwargs}\")\n",
      "        start_time = time.time()\n",
      "        result = func(*args, **kwargs)\n",
      "        end_time = time.time()\n",
      "        print(f\"LLM call completed in {end_time - start_time:.2f} seconds.\")\n",
      "        print(f\"Result: {result}\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "# Wrap the LLM call with the decorator\n",
      "@log_llm_calls\n",
      "def call_openai_llm(prompt):\n",
      "    llm = OpenAI(model=\"text-davinci-003\", temperature=0.7)\n",
      "    return llm(prompt)\n",
      "\n",
      "# Use the decorated function\n",
      "response = call_openai_llm(\"What are the benefits of Python decorators?\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Calling LLM with args: ('What are the benefits of Python decorators?',), kwargs: {}\n",
      "LLM call completed in 0.89 seconds.\n",
      "Result: Python decorators allow you to modify or enhance the behavior of functions without changing their code...\n",
      "```\n",
      "\n",
      "This approach helps you monitor how and when your application interacts with the LLM and debug issues more easily.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Customizing Chain Behavior**\n",
      "In LangChain, **chains** are sequences of operations. A decorator can modify the input/output behavior of a chain or add functionality like logging, caching, error handling, or profiling.\n",
      "\n",
      "#### Example: Logging Inputs and Outputs of a Chain\n",
      "```python\n",
      "from langchain.chains import SimpleSequentialChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# Define a decorator to log chain inputs and outputs\n",
      "def log_chain_io(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(f\"Input to chain: {args[1:]}\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(f\"Output from chain: {result}\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "# Define two simple prompts\n",
      "prompt1 = PromptTemplate(input_variables=[\"name\"], template=\"What are some fun facts about {name}?\")\n",
      "prompt2 = PromptTemplate(input_variables=[\"facts\"], template=\"Can you summarize these facts: {facts}?\")\n",
      "\n",
      "# Define the LLM\n",
      "llm = OpenAI(model=\"text-davinci-003\", temperature=0.7)\n",
      "\n",
      "# Create a chain\n",
      "@log_chain_io\n",
      "def create_chain():\n",
      "    chain1 = SimpleSequentialChain(llm=llm, prompt=prompt1)\n",
      "    chain2 = SimpleSequentialChain(llm=llm, prompt=prompt2)\n",
      "    return SimpleSequentialChain(chains=[chain1, chain2])\n",
      "\n",
      "# Run the chain\n",
      "chain = create_chain()\n",
      "result = chain.run(\"Albert Einstein\")\n",
      "```\n",
      "\n",
      "The decorator logs the inputs and outputs of each step in the chain, which is especially helpful for debugging complex workflows.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Tool Execution Tracking**\n",
      "In LangChain, **tools** are functions that an agent can call. You can use decorators to track when tools are invoked, what inputs they take, and what outputs they produce.\n",
      "\n",
      "#### Example: Tracking Tool Execution\n",
      "```python\n",
      "from langchain.agents import Tool\n",
      "\n",
      "# Define a decorator to track tool execution\n",
      "def track_tool_execution(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(f\"Tool '{func.__name__}' is being executed with args: {args}, kwargs: {kwargs}\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(f\"Tool '{func.__name__}' execution completed. Result: {result}\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "# Define a tool\n",
      "@track_tool_execution\n",
      "def weather_tool(location: str) -> str:\n",
      "    # Simulated API call\n",
      "    return f\"The weather in {location} is sunny.\"\n",
      "\n",
      "# Wrap the tool in a LangChain Tool object\n",
      "tool = Tool(\n",
      "    name=\"WeatherTool\",\n",
      "    func=weather_tool,\n",
      "    description=\"Provides the current weather for a given location.\"\n",
      ")\n",
      "\n",
      "# Simulate tool usage\n",
      "tool(\"New York\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Tool 'weather_tool' is being executed with args: ('New York',), kwargs: {}\n",
      "Tool 'weather_tool' execution completed. Result: The weather in New York is sunny.\n",
      "```\n",
      "\n",
      "This can be useful for debugging or monitoring how and when tools are used by agents in your LangChain workflows.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Retry Logic for LLM Requests**\n",
      "When working with LLMs or external APIs, requests may occasionally fail due to network issues or rate limits. A decorator can automatically retry a failed request.\n",
      "\n",
      "#### Example: Retry Decorator\n",
      "```python\n",
      "import time\n",
      "from langchain.llms import OpenAI\n",
      "from requests.exceptions import RequestException\n",
      "\n",
      "# Define a retry decorator\n",
      "def retry_decorator(retries=3, delay=2):\n",
      "    def decorator(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            for attempt in range(retries):\n",
      "                try:\n",
      "                    return func(*args, **kwargs)\n",
      "                except RequestException as e:\n",
      "                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n",
      "                    time.sleep(delay)\n",
      "            raise Exception(\"All retries failed.\")\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "# Apply the decorator to an LLM call\n",
      "@retry_decorator(retries=3, delay=1)\n",
      "def call_llm_with_retry(prompt):\n",
      "    llm = OpenAI(model=\"text-davinci-003\", temperature=0.7)\n",
      "    return llm(prompt)\n",
      "\n",
      "# Use the decorated function\n",
      "try:\n",
      "    response = call_llm_with_retry(\"Tell me a joke.\")\n",
      "    print(response)\n",
      "except Exception as e:\n",
      "    print(e)\n",
      "```\n",
      "\n",
      "This ensures your LangChain application gracefully handles transient errors and retries LLM calls.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Custom Agents**\n",
      "LangChain allows you to create **custom agents**. Decorators can enhance the agent's decision-making process, modify agent inputs or outputs, or log its behavior.\n",
      "\n",
      "#### Example: Logging Agent Decisions\n",
      "```python\n",
      "from langchain.agents import AgentExecutor\n",
      "\n",
      "# Define a decorator to log agent actions\n",
      "def log_agent_actions(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(\"Agent is making a decision...\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(f\"Agent decided: {result}\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "# Wrap an AgentExecutor's run method\n",
      "@log_agent_actions\n",
      "def custom_agent_run(agent, input):\n",
      "    return agent.run(input)\n",
      "\n",
      "# Example usage\n",
      "# agent = create_your_agent()\n",
      "# response = custom_agent_run(agent, \"What is the weather in Paris?\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Summary: How Decorators Help in LangChain\n",
      "1. **Logging:** Track LLM calls, chain inputs/outputs, tool usage, etc.\n",
      "2. **Retries:** Implement retry logic for API/LLM requests.\n",
      "3. **Profiling:** Measure execution time of chains, tools, or LLM calls.\n",
      "4. **Custom Enhancements:** Add custom functionality to agents, tools, or chains dynamically.\n",
      "5. **Error Handling:** Gracefully handle and log errors in workflows.\n",
      "\n",
      "Decorators in Python integrate seamlessly with LangChain's modular components, allowing you to build robust, efficient, and easily debuggable applications. Let me know if you'd like to see a specific use case in more detail!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create memory\n",
    "qa_history = ChatMessageHistory()\n",
    "\n",
    "# Create a prompt with memory placeholder\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a knowledgeable AI assistant specialized in Python programming and LangChain. \"\n",
    "               \"Answer questions clearly and provide code examples when helpful.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "# Wrap with history\n",
    "qa_with_history = RunnableWithMessageHistory(\n",
    "    qa_chain,\n",
    "    lambda session_id: qa_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"What is a decorator in Python?\",\n",
    "    \"Can you show me an example?\",\n",
    "    \"How does this relate to LangChain?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {q}\")\n",
    "    print('='*60)\n",
    "    response = qa_with_history.invoke(\n",
    "        {\"question\": q},\n",
    "        config={\"configurable\": {\"session_id\": \"qa_session\"}}\n",
    "    )\n",
    "    print(f\"\\nAnswer: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f45ff",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Now that you've learned the fundamentals, here are some advanced topics to explore:\n",
    "\n",
    "1. **Document Loaders and Vector Stores**\n",
    "   - Load and process documents\n",
    "   - Create embeddings\n",
    "   - Build RAG (Retrieval Augmented Generation) systems\n",
    "\n",
    "2. **Advanced Agents**\n",
    "   - Custom agent types\n",
    "   - Tool integration (web search, APIs, databases)\n",
    "   - Multi-agent systems\n",
    "\n",
    "3. **LangSmith**\n",
    "   - Debugging and monitoring\n",
    "   - Prompt optimization\n",
    "   - Performance tracking\n",
    "\n",
    "4. **LangServe**\n",
    "   - Deploy LangChain applications as APIs\n",
    "   - Production-ready serving\n",
    "\n",
    "5. **Integration with Vector Databases**\n",
    "   - Pinecone, Weaviate, Chroma\n",
    "   - Semantic search\n",
    "   - Long-term memory\n",
    "\n",
    "### Resources\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "- [LangChain Discord Community](https://discord.gg/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! ðŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420abd19",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Tips\n",
    "\n",
    "Here are some key takeaways for working with LangChain:\n",
    "\n",
    "### 1. **Start Simple, Then Scale**\n",
    "- Begin with basic chains before implementing complex agents\n",
    "- Test each component individually before combining them\n",
    "\n",
    "### 2. **Memory Management**\n",
    "- Choose the right memory type for your use case:\n",
    "  - `ConversationBufferMemory`: Full history (good for short conversations)\n",
    "  - `ConversationBufferWindowMemory`: Limited history (better for long conversations)\n",
    "  - `ConversationSummaryMemory`: Summarized history (best for cost optimization)\n",
    "\n",
    "### 3. **Prompt Engineering**\n",
    "- Be specific in your prompts\n",
    "- Use system messages to set context and behavior\n",
    "- Include examples in your prompts (few-shot learning)\n",
    "\n",
    "### 4. **Error Handling**\n",
    "- Always handle potential errors in chains and agents\n",
    "- Use `handle_parsing_errors=True` for agents\n",
    "- Implement retry logic for API calls\n",
    "\n",
    "### 5. **Cost Optimization**\n",
    "- Monitor token usage\n",
    "- Use caching when appropriate\n",
    "- Choose the right model for your task (GPT-4 for complex, GPT-3.5 for simple tasks)\n",
    "\n",
    "### 6. **Testing**\n",
    "- Test prompts with various inputs\n",
    "- Validate agent tool selections\n",
    "- Monitor chain outputs for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edca1b",
   "metadata": {},
   "source": [
    "## 8. Practical Example: Building a Q&A System\n",
    "\n",
    "Let's build a complete question-answering system with memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48f819",
   "metadata": {},
   "source": [
    "## 7. Output Parsers\n",
    "\n",
    "Output parsers help structure the LLM's responses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520f84c",
   "metadata": {},
   "source": [
    "## 5. Memory - Maintaining Conversation Context\n",
    "\n",
    "Memory allows your application to remember previous interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Unlock the Power of AI with LangChain: A Game-Changer for Developers**\n",
      "\n",
      "In the ever-evolving world of artificial intelligence, building applications that leverage advanced language models has become a critical focus for developers across industries. Enter **LangChain**, a powerful framework designed to simplify and supercharge the process of creating AI applications by chaining together language model interactions. Whether you're building chatbots, summarization tools, or complex decision-making systems, LangChain provides a robust solution to streamline development and maximize efficiency.\n",
      "\n",
      "### What Is LangChain?\n",
      "\n",
      "LangChain is an open-source framework that allows developers to integrate language models like OpenAIâ€™s GPT or similar models into applications with ease. But it doesnâ€™t stop thereâ€”it enables developers to chain together multiple operations, combine external data sources, and create dynamic, context-aware applications. The flexibility and modular nature of LangChain make it an indispensable tool for anyone working on applications powered by natural language processing (NLP).\n",
      "\n",
      "### Key Benefits of Using LangChain\n",
      "\n",
      "1. **Simplified Development Process**  \n",
      "   LangChain eliminates the complexity of manually managing multiple language model interactions. It provides pre-built components, such as prompt templates, memory modules, and chains, allowing developers to focus on application logic rather than low-level implementation details.\n",
      "\n",
      "2. **Enhanced Functionality with External Data**  \n",
      "   LangChain supports integrations with external data sources, APIs, and databases. This capability means you can build applications that combine pre-trained language models with real-time, context-sensitive informationâ€”ideal for creating dynamic and adaptive AI solutions.\n",
      "\n",
      "3. **Modularity and Scalability**  \n",
      "   LangChain is highly modular, enabling developers to build and scale applications with ease. Whether you need single-step language model interactions or multi-step workflows, LangChainâ€™s architecture adapts to your needs.\n",
      "\n",
      "4. **Memory and Context Management**  \n",
      "   One of LangChainâ€™s standout features is its ability to manage memory and context effectively. For example, it can retain context across conversations, making it perfect for applications like personalized chatbots, customer support tools, or educational assistants.\n",
      "\n",
      "5. **Customizability and Flexibility**  \n",
      "   LangChain empowers developers to design tailored solutions. From crafting sophisticated prompts to creating chains for specific workflows, the framework adapts to varying requirements without locking you into rigid templates.\n",
      "\n",
      "6. **Community and Ecosystem Support**  \n",
      "   As an open-source project, LangChain boasts an active community of developers and contributors. This vibrant ecosystem provides abundant resources, tutorials, and tools to accelerate development and troubleshoot challenges.\n",
      "\n",
      "### Why LangChain Matters\n",
      "\n",
      "The ability to chain together interactions with language models unlocks new possibilities for AI applications. LangChainâ€™s focus on modularity, external data integration, and memory management allows developers to move beyond simple queries and build intelligent systems that feel intuitive and responsive. For businesses and individuals looking to harness the power of AI, LangChain offers a clear path to innovation and success.\n",
      "\n",
      "### Getting Started with LangChain\n",
      "\n",
      "Ready to dive in? The LangChain documentation is comprehensive and beginner-friendly, making it easy to explore its features and start building. Whether youâ€™re a seasoned AI expert or a newcomer to NLP, LangChain is the key to creating smarter, more impactful applications with language models.\n",
      "\n",
      "---\n",
      "\n",
      "LangChain is more than just a toolâ€”itâ€™s a catalyst for creative, intelligent AI solutions. By simplifying workflows, enhancing functionality, and enabling dynamic applications, LangChain is paving the way for the next generation of AI-driven innovation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple chain using LCEL (LangChain Expression Language)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class technical writer.\"),\n",
    "    (\"user\", \"Write a short blog post about {topic}\")\n",
    "])\n",
    "\n",
    "# Chain: prompt -> llm -> output parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"topic\": \"the benefits of using LangChain\"})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
