{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f979f1f2",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals Tutorial\n",
    "\n",
    "Welcome to this comprehensive guide on **LangChain** - a powerful framework for building applications with Large Language Models (LLMs).\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "LangChain is a framework designed to simplify the creation of applications using large language models. It provides:\n",
    "- **Modular components** for working with LLMs\n",
    "- **Chains** to combine multiple components\n",
    "- **Memory** to maintain conversation context\n",
    "- **Agents** for autonomous decision-making\n",
    "- **Tools** for external integrations\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Setting up LangChain\n",
    "2. Working with LLMs and Chat Models\n",
    "3. Prompt Templates\n",
    "4. Chains\n",
    "5. Memory Management\n",
    "6. Agents and Tools\n",
    "7. Practical Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c26136f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell only once\n",
    "# !pip install langchain langchain-openai langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fc964",
   "metadata": {},
   "source": [
    "**Note:** Create a `.env` file in your project root with your API keys:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "# or for Azure OpenAI\n",
    "AZURE_OPENAI_API_KEY=your_azure_key\n",
    "AZURE_OPENAI_ENDPOINT=your_azure_endpoint\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec44de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55539a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed for building applications using language models. It revolves around three main concepts that help developers create powerful, flexible, and efficient applications. The three main concepts are:\n",
      "\n",
      "1. **Chains**: Chains involve sequences of actions or steps that process and transform data by leveraging language models. They can be simple (e.g., a single prompt to an LLM) or complex (e.g., multi-step workflows combining multiple models, APIs, or data sources). Chains enable the automation of tasks and the orchestration of various components to achieve desired outcomes.\n",
      "\n",
      "2. **Agents**: Agents add dynamic decision-making capabilities to applications. Unlike predefined chains, agents have access to tools and can decide which action to take based on the given context. For example, an agent might use an LLM to determine whether to query a database, call an API, or ask another question. This allows for more flexible and adaptable applications.\n",
      "\n",
      "3. **Memory**: Memory provides stateful behavior by enabling applications to store and retrieve context over time. For instance, memory can be used to keep track of user interactions, maintain conversational history, or retain data for multi-turn conversations. This helps create more personalized and context-aware applications.\n",
      "\n",
      "These concepts work together to empower developers to build sophisticated applications that leverage the capabilities of language models effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Initialize Azure OpenAI Chat Model\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://lnguyentuan-azure-openai.openai.azure.com/\"),\n",
    "    api_key=os.getenv(\"KEY\"),\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Simple invocation\n",
    "response = llm.invoke(\"What are the three main concepts in LangChain?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a6693a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Prompt:\n",
      "Explain machine learning in simple terms for a beginner.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Chat Prompt:\n",
      "SystemMessage: You are a helpful AI assistant that explains technical concepts clearly.\n",
      "HumanMessage: Explain neural networks to a high school student.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Simple Prompt Template\n",
    "simple_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms for a beginner.\"\n",
    ")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = simple_template.format(topic=\"machine learning\")\n",
    "print(\"Simple Prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Chat Prompt Template (for chat models)\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant that explains technical concepts clearly.\"),\n",
    "    (\"user\", \"Explain {topic} to a {audience}.\")\n",
    "])\n",
    "\n",
    "# Format the chat prompt\n",
    "formatted_chat = chat_template.format_messages(\n",
    "    topic=\"neural networks\",\n",
    "    audience=\"high school student\"\n",
    ")\n",
    "print(\"Chat Prompt:\")\n",
    "for msg in formatted_chat:\n",
    "    print(f\"{msg.__class__.__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b28a4",
   "metadata": {},
   "source": [
    "## 4. Chains - Combining Components\n",
    "\n",
    "Chains allow you to combine multiple components (prompts, LLMs, and other chains) into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83185dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Response:\n",
      "Sure! Letâ€™s break it down step by step in simple terms:\n",
      "\n",
      "### 1. What is a neural network?\n",
      "A **neural network** is a type of computer program designed to learn and make decisions, kind of like how your brain works. It's used in artificial intelligence (AI) to solve problems, like recognizing images, translating languages, or predicting what might happen next.\n",
      "\n",
      "### 2. Why is it called a \"neural\" network?\n",
      "The name comes from how it's inspired by the human brain. Your brain is full of billions of cells called **neurons** that are connected and communicate with each other. A neural network tries to copy how these neurons work using math and computer code.\n",
      "\n",
      "### 3. How does it work?\n",
      "Imagine you have a task, like recognizing whether a picture is of a dog or a cat. Hereâ€™s how a neural network would tackle this:\n",
      "\n",
      "#### a) **Inputs**: \n",
      "Every picture is made of pixels (tiny dots of color). These pixels are fed into the neural network as numbers â€“ the input.\n",
      "\n",
      "#### b) **Hidden layers**:\n",
      "Inside the network, there are layers of \"virtual neurons.\" These layers process the input (the numbers from the pixels). There can be many hidden layers that work together to figure out patterns, like shapes, edges, or colors in the picture.\n",
      "\n",
      "#### c) **Outputs**:\n",
      "At the end, the network gives an answer â€“ for example, \"dog\" or \"cat.\" The output depends on what the neural network has learned.\n",
      "\n",
      "### 4. How does the network learn?\n",
      "Neural networks learn by practice, just like you do. Hereâ€™s how:\n",
      "\n",
      "#### a) **Training with data**:\n",
      "You show the network lots of pictures of dogs and cats (this is called training data). For each picture, you tell the network the correct answer so it can learn.\n",
      "\n",
      "#### b) **Mistakes and adjustments**:\n",
      "At first, the network makes mistakes (maybe it guesses \"dog\" for a cat). But every time it makes a mistake, it adjusts itself using math to get better.\n",
      "\n",
      "#### c) **Getting better over time**:\n",
      "The more pictures it sees, the more accurate it becomes at telling a dog from a cat. This process is called **learning**.\n",
      "\n",
      "### 5. Why is it cool?\n",
      "Neural networks are super powerful because they can learn to understand really complex problems. For example:\n",
      "- They can help doctors spot diseases in X-ray images.\n",
      "- They can drive cars by recognizing roads, traffic signs, and obstacles.\n",
      "- They can even create art, write stories, or predict the weather!\n",
      "\n",
      "### 6. What does it look like?\n",
      "If you were to draw a neural network, it would look like a web of connected dots:\n",
      "- **Input layer**: The first layer of dots (neurons) takes in the raw data, like pixels of a picture.\n",
      "- **Hidden layers**: These are the middle layers where the network does its calculations and finds patterns.\n",
      "- **Output layer**: The final layer gives the answer â€“ like \"dog\" or \"cat.\"\n",
      "\n",
      "### 7. Key idea: It's like learning patterns\n",
      "Think of it like learning to tell apples from oranges. At first, you might look at size, color, or shape. Over time, you get better at spotting the differences. Neural networks do something similar, but they use math and data to figure out the patterns.\n",
      "\n",
      "### Conclusion:\n",
      "Neural networks are tools that help computers solve problems by mimicking how brains work. They learn from examples, improve over time, and can solve really complex tasks. They're behind many of the technologies you use every day, like face recognition, virtual assistants, and recommendation systems (like Netflix suggesting movies).\n",
      "\n",
      "Does that make sense? Would you like an example or analogy?\n"
     ]
    }
   ],
   "source": [
    "# Use the prompt template with LLM\n",
    "response = llm.invoke(formatted_chat)\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d738937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **AI as a Collaborative Partner in Creativity:** AI is not just a tool but increasingly a collaborator in artistic endeavors. For example, generative AI models like OpenAI's DALLÂ·E or music composition AI such as AIVA are enabling artists, musicians, and writers to push their creative boundaries by co-creating works. These systems can generate ideas, assist with execution, and even inspire new forms of artistic expression, demonstrating how humans and machines can work together to produce innovative results.\n",
      "\n",
      "2. **Challenging Traditional Notions of Authorship and Originality:** AI-created art raises profound questions about what it means to be an \"author\" or \"creator.\" When a machine generates a painting, composes music, or writes a poem, who owns the intellectual property? Is it the AI, the programmer, or the person who provided the input or prompt? These questions are reshaping legal and philosophical discussions about creativity and innovation in the digital age.\n",
      "\n",
      "3. **Ethical and Cultural Implications of AI Art:** While AI is revolutionizing creativity, it is also sparking debates about its ethical impact. Critics argue that AI-generated content could undermine human artists by competing in markets traditionally dominated by human talent. Additionally, concerns about the authenticity and emotional depth of AI-created works challenge societal perceptions of art's role as a reflection of human experience. These discussions highlight the need for a balance between embracing technological advancements and preserving the unique qualities of human creativity.\n"
     ]
    }
   ],
   "source": [
    "# Sequential Chain Example - Multiple steps\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "topic_prompt = ChatPromptTemplate.from_template(\"Suggest a creative topic about {subject}\")\n",
    "topic_chain = topic_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Write about that topic\n",
    "writing_prompt = ChatPromptTemplate.from_template(\"Write 3 interesting facts about: {topic}\")\n",
    "writing_chain = writing_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine them\n",
    "full_chain = {\"topic\": topic_chain} | writing_chain\n",
    "\n",
    "result = full_chain.invoke({\"subject\": \"artificial intelligence\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c242810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question:\n",
      "Assistant: Hi Alex! That's awesome to hear that you love programming! What kind of programming do you enjoy most? Are you into web development, game development, data science, or something else entirely?\n",
      "\n",
      "\n",
      "Second question:\n",
      "Assistant: Hi Alex! That's awesome to hear that you love programming! What kind of programming do you enjoy most? Are you into web development, game development, data science, or something else entirely?\n",
      "\n",
      "\n",
      "Second question:\n",
      "Assistant: Your name is Alex! ðŸ˜Š\n",
      "\n",
      "\n",
      "Third question:\n",
      "Assistant: Your name is Alex! ðŸ˜Š\n",
      "\n",
      "\n",
      "Third question:\n",
      "Assistant: You love programming! ðŸ˜ŠðŸ’»\n",
      "Assistant: You love programming! ðŸ˜ŠðŸ’»\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create message history\n",
    "message_history = ChatMessageHistory()\n",
    "\n",
    "# Create a prompt with memory\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chain with memory\n",
    "chain = prompt | llm\n",
    "\n",
    "# Wrapper to handle history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "print(\"First question:\")\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi! My name is Alex and I love programming.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(f\"Assistant: {response1.content}\\n\")\n",
    "\n",
    "# Second interaction - the model should remember\n",
    "print(\"\\nSecond question:\")\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(f\"Assistant: {response2.content}\\n\")\n",
    "\n",
    "# Third interaction\n",
    "print(\"\\nThird question:\")\n",
    "response3 = chain_with_history.invoke(\n",
    "    {\"input\": \"What do I love?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo\"}}\n",
    ")\n",
    "print(f\"Assistant: {response3.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d93edf",
   "metadata": {},
   "source": [
    "## 6. Agents and Tools\n",
    "\n",
    "Agents can decide which tools to use based on the user's input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5449602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current conversation history:\n",
      "Messages in history: 6\n",
      "human: Hi! My name is Alex and I love programming.\n",
      "ai: Hi Alex! That's awesome to hear that you love programming! What kind of programming do you enjoy most? Are you into web development, game development, data science, or something else entirely?\n",
      "human: What's my name?\n",
      "ai: Your name is Alex! ðŸ˜Š\n",
      "human: What do I love?\n",
      "ai: You love programming! ðŸ˜ŠðŸ’»\n"
     ]
    }
   ],
   "source": [
    "# View current memory\n",
    "print(\"Current conversation history:\")\n",
    "print(f\"Messages in history: {len(message_history.messages)}\")\n",
    "for msg in message_history.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b17f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "  - get_word_length: Returns the length of a word.\n",
      "  - multiply: Multiplies two numbers together.\n",
      "\n",
      "==================================================\n",
      "Tool Test 1: Finding word length\n",
      "The word 'LangChain' has 9 letters\n",
      "\n",
      "==================================================\n",
      "Tool Test 2: Multiplication\n",
      "15 Ã— 7 = 105\n",
      "\n",
      "==================================================\n",
      "\n",
      "Note: For full agent capabilities with tool calling, you would need:\n",
      "1. An LLM model that supports function/tool calling (like GPT-4)\n",
      "2. The langgraph package for advanced agent orchestration\n",
      "3. Example: from langgraph.prebuilt import create_react_agent\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define custom tools using decorator\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Create tools list\n",
    "tools = [get_word_length, multiply]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description}\")\n",
    "\n",
    "# Simple demonstration of tool usage\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Tool Test 1: Finding word length\")\n",
    "word_to_test = \"LangChain\"\n",
    "result = get_word_length.invoke(word_to_test)\n",
    "print(f\"The word '{word_to_test}' has {result} letters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Tool Test 2: Multiplication\")\n",
    "result = multiply.invoke({\"a\": 15, \"b\": 7})\n",
    "print(f\"15 Ã— 7 = {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nNote: For full agent capabilities with tool calling, you would need:\")\n",
    "print(\"1. An LLM model that supports function/tool calling (like GPT-4)\")\n",
    "print(\"2. The langgraph package for advanced agent orchestration\")\n",
    "print(\"3. Example: from langgraph.prebuilt import create_react_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "945b4845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON output:\n",
      "{'title': '1984', 'author': 'George Orwell', 'year': 1949, 'genre': 'Dystopian'}\n",
      "\n",
      "Title: 1984\n",
      "Author: George Orwell\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define expected output structure\n",
    "class Book(BaseModel):\n",
    "    title: str = Field(description=\"The title of the book\")\n",
    "    author: str = Field(description=\"The author of the book\")\n",
    "    year: int = Field(description=\"The year of publication\")\n",
    "    genre: str = Field(description=\"The genre of the book\")\n",
    "\n",
    "# Create parser\n",
    "parser = JsonOutputParser(pydantic_object=Book)\n",
    "\n",
    "# Create prompt with format instructions\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that outputs JSON.\"),\n",
    "    (\"user\", \"{query}\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\n",
    "    \"query\": \"Tell me about the book '1984'\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(\"Parsed JSON output:\")\n",
    "print(result)\n",
    "print(f\"\\nTitle: {result['title']}\")\n",
    "print(f\"Author: {result['author']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b3740d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: What is a decorator in Python?\n",
      "============================================================\n",
      "\n",
      "Answer: A **decorator** in Python is a design pattern that allows you to modify or extend the behavior of a function or method without changing its actual code. Decorators use higher-order functions (functions that take other functions as arguments) to wrap additional functionality around the original function.\n",
      "\n",
      "In Python, decorators are commonly used for tasks like logging, enforcing access control, memoization, or pre/post-processing. Decorators are applied to functions using the `@` syntax.\n",
      "\n",
      "### Example: Basic Decorator\n",
      "Hereâ€™s an example of how a decorator works:\n",
      "\n",
      "```python\n",
      "# Define a decorator function\n",
      "def my_decorator(func):\n",
      "    def wrapper():\n",
      "        print(\"Something is happening before the function is called.\")\n",
      "        func()  # Call the original function\n",
      "        print(\"Something is happening after the function is called.\")\n",
      "    return wrapper\n",
      "\n",
      "# Apply the decorator to a function\n",
      "@my_decorator\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "\n",
      "# Call the decorated function\n",
      "say_hello()\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Something is happening before the function is called.\n",
      "Hello!\n",
      "Something is happening after the function is called.\n",
      "```\n",
      "\n",
      "### How It Works:\n",
      "1. The `@my_decorator` syntax is shorthand for `say_hello = my_decorator(say_hello)`.\n",
      "2. The `my_decorator` function takes `say_hello` as an argument and returns a new function (`wrapper`) that wraps additional behavior around the original `say_hello` function.\n",
      "3. When `say_hello` is called, it executes the logic defined in the `wrapper` function.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Decorator with Arguments\n",
      "You can also create decorators that accept arguments by adding another level of function nesting:\n",
      "\n",
      "```python\n",
      "def repeat(num_times):\n",
      "    def decorator(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            for _ in range(num_times):\n",
      "                func(*args, **kwargs)\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "@repeat(num_times=3)\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "\n",
      "# Call the decorated function\n",
      "greet(\"Alice\")\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Hello, Alice!\n",
      "Hello, Alice!\n",
      "Hello, Alice!\n",
      "```\n",
      "\n",
      "### Key Points:\n",
      "- Decorators are applied at the time of function definition, not when the function is executed.\n",
      "- You can stack multiple decorators by applying them one after another (e.g., `@decorator1`, `@decorator2`).\n",
      "- Decorators can modify or replace the original function, enabling flexible programming patterns.\n",
      "\n",
      "Let me know if you'd like more advanced examples or use cases!\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: Can you show me an example?\n",
      "============================================================\n",
      "\n",
      "Answer: A **decorator** in Python is a design pattern that allows you to modify or extend the behavior of a function or method without changing its actual code. Decorators use higher-order functions (functions that take other functions as arguments) to wrap additional functionality around the original function.\n",
      "\n",
      "In Python, decorators are commonly used for tasks like logging, enforcing access control, memoization, or pre/post-processing. Decorators are applied to functions using the `@` syntax.\n",
      "\n",
      "### Example: Basic Decorator\n",
      "Hereâ€™s an example of how a decorator works:\n",
      "\n",
      "```python\n",
      "# Define a decorator function\n",
      "def my_decorator(func):\n",
      "    def wrapper():\n",
      "        print(\"Something is happening before the function is called.\")\n",
      "        func()  # Call the original function\n",
      "        print(\"Something is happening after the function is called.\")\n",
      "    return wrapper\n",
      "\n",
      "# Apply the decorator to a function\n",
      "@my_decorator\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "\n",
      "# Call the decorated function\n",
      "say_hello()\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Something is happening before the function is called.\n",
      "Hello!\n",
      "Something is happening after the function is called.\n",
      "```\n",
      "\n",
      "### How It Works:\n",
      "1. The `@my_decorator` syntax is shorthand for `say_hello = my_decorator(say_hello)`.\n",
      "2. The `my_decorator` function takes `say_hello` as an argument and returns a new function (`wrapper`) that wraps additional behavior around the original `say_hello` function.\n",
      "3. When `say_hello` is called, it executes the logic defined in the `wrapper` function.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Decorator with Arguments\n",
      "You can also create decorators that accept arguments by adding another level of function nesting:\n",
      "\n",
      "```python\n",
      "def repeat(num_times):\n",
      "    def decorator(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            for _ in range(num_times):\n",
      "                func(*args, **kwargs)\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "@repeat(num_times=3)\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "\n",
      "# Call the decorated function\n",
      "greet(\"Alice\")\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Hello, Alice!\n",
      "Hello, Alice!\n",
      "Hello, Alice!\n",
      "```\n",
      "\n",
      "### Key Points:\n",
      "- Decorators are applied at the time of function definition, not when the function is executed.\n",
      "- You can stack multiple decorators by applying them one after another (e.g., `@decorator1`, `@decorator2`).\n",
      "- Decorators can modify or replace the original function, enabling flexible programming patterns.\n",
      "\n",
      "Let me know if you'd like more advanced examples or use cases!\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: Can you show me an example?\n",
      "============================================================\n",
      "\n",
      "Answer: Sure! Hereâ€™s a practical example of using a decorator to log the execution of a function. This is a common use case for decorators.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Logging Decorator\n",
      "\n",
      "```python\n",
      "import time\n",
      "\n",
      "# Define the decorator function\n",
      "def log_execution_time(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        start_time = time.time()  # Record the start time\n",
      "        result = func(*args, **kwargs)  # Call the original function\n",
      "        end_time = time.time()  # Record the end time\n",
      "        print(f\"Function '{func.__name__}' executed in {end_time - start_time:.4f} seconds.\")\n",
      "        return result  # Return the result of the original function\n",
      "    return wrapper\n",
      "\n",
      "# Use the decorator\n",
      "@log_execution_time\n",
      "def compute_sum(n):\n",
      "    \"\"\"Compute the sum of numbers from 1 to n.\"\"\"\n",
      "    return sum(range(1, n + 1))\n",
      "\n",
      "# Call the function\n",
      "result = compute_sum(1000000)\n",
      "print(f\"Result: {result}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Function 'compute_sum' executed in 0.0431 seconds.\n",
      "Result: 500000500000\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Decorator Function (`log_execution_time`)**:\n",
      "   - It wraps the original function (`compute_sum`) inside another function (`wrapper`).\n",
      "   - Inside `wrapper`, it calculates the execution time of the original function and prints it.\n",
      "\n",
      "2. **`@log_execution_time`**:\n",
      "   - This applies the decorator to `compute_sum` at definition time.\n",
      "   - Essentially, `compute_sum` is replaced with `log_execution_time(compute_sum)`.\n",
      "\n",
      "3. **Result**:\n",
      "   - When you call `compute_sum(1000000)`, the decorator logs the execution time and returns the result of the original function.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Multiple Decorators\n",
      "\n",
      "You can also stack multiple decorators. Hereâ€™s an example where we log the execution time and validate the input:\n",
      "\n",
      "```python\n",
      "# Decorator to validate input\n",
      "def validate_input(func):\n",
      "    def wrapper(n):\n",
      "        if n < 0:\n",
      "            raise ValueError(\"Input must be a non-negative integer.\")\n",
      "        return func(n)\n",
      "    return wrapper\n",
      "\n",
      "# Use both decorators\n",
      "@log_execution_time\n",
      "@validate_input\n",
      "def compute_factorial(n):\n",
      "    \"\"\"Compute the factorial of n.\"\"\"\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    return n * compute_factorial(n - 1)\n",
      "\n",
      "# Call the function\n",
      "try:\n",
      "    print(compute_factorial(10))  # Valid input\n",
      "    print(compute_factorial(-5))  # Invalid input (will raise an error)\n",
      "except ValueError as e:\n",
      "    print(e)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Function 'compute_factorial' executed in 0.0000 seconds.\n",
      "3628800\n",
      "Input must be a non-negative integer.\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **`@validate_input`** ensures that the input is valid before calling the original function.\n",
      "- **`@log_execution_time`** logs the time taken to compute the factorial.\n",
      "- The two decorators work together in sequence: first `validate_input`, then `log_execution_time`.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you want further clarification or more examples!\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: How does this relate to LangChain?\n",
      "============================================================\n",
      "\n",
      "Answer: Sure! Hereâ€™s a practical example of using a decorator to log the execution of a function. This is a common use case for decorators.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Logging Decorator\n",
      "\n",
      "```python\n",
      "import time\n",
      "\n",
      "# Define the decorator function\n",
      "def log_execution_time(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        start_time = time.time()  # Record the start time\n",
      "        result = func(*args, **kwargs)  # Call the original function\n",
      "        end_time = time.time()  # Record the end time\n",
      "        print(f\"Function '{func.__name__}' executed in {end_time - start_time:.4f} seconds.\")\n",
      "        return result  # Return the result of the original function\n",
      "    return wrapper\n",
      "\n",
      "# Use the decorator\n",
      "@log_execution_time\n",
      "def compute_sum(n):\n",
      "    \"\"\"Compute the sum of numbers from 1 to n.\"\"\"\n",
      "    return sum(range(1, n + 1))\n",
      "\n",
      "# Call the function\n",
      "result = compute_sum(1000000)\n",
      "print(f\"Result: {result}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Function 'compute_sum' executed in 0.0431 seconds.\n",
      "Result: 500000500000\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Decorator Function (`log_execution_time`)**:\n",
      "   - It wraps the original function (`compute_sum`) inside another function (`wrapper`).\n",
      "   - Inside `wrapper`, it calculates the execution time of the original function and prints it.\n",
      "\n",
      "2. **`@log_execution_time`**:\n",
      "   - This applies the decorator to `compute_sum` at definition time.\n",
      "   - Essentially, `compute_sum` is replaced with `log_execution_time(compute_sum)`.\n",
      "\n",
      "3. **Result**:\n",
      "   - When you call `compute_sum(1000000)`, the decorator logs the execution time and returns the result of the original function.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Multiple Decorators\n",
      "\n",
      "You can also stack multiple decorators. Hereâ€™s an example where we log the execution time and validate the input:\n",
      "\n",
      "```python\n",
      "# Decorator to validate input\n",
      "def validate_input(func):\n",
      "    def wrapper(n):\n",
      "        if n < 0:\n",
      "            raise ValueError(\"Input must be a non-negative integer.\")\n",
      "        return func(n)\n",
      "    return wrapper\n",
      "\n",
      "# Use both decorators\n",
      "@log_execution_time\n",
      "@validate_input\n",
      "def compute_factorial(n):\n",
      "    \"\"\"Compute the factorial of n.\"\"\"\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    return n * compute_factorial(n - 1)\n",
      "\n",
      "# Call the function\n",
      "try:\n",
      "    print(compute_factorial(10))  # Valid input\n",
      "    print(compute_factorial(-5))  # Invalid input (will raise an error)\n",
      "except ValueError as e:\n",
      "    print(e)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Function 'compute_factorial' executed in 0.0000 seconds.\n",
      "3628800\n",
      "Input must be a non-negative integer.\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **`@validate_input`** ensures that the input is valid before calling the original function.\n",
      "- **`@log_execution_time`** logs the time taken to compute the factorial.\n",
      "- The two decorators work together in sequence: first `validate_input`, then `log_execution_time`.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you want further clarification or more examples!\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question: How does this relate to LangChain?\n",
      "============================================================\n",
      "\n",
      "Answer: Great question! In LangChain, decorators are heavily utilized to manage workflows, track execution, log events, and enhance the functionality of chains, agents, and tools.\n",
      "\n",
      "LangChain is a framework for building applications powered by large language models (LLMs), and it often involves complex interactions between various components (e.g., chains of prompts, tools, agents). Decorators can play a vital role in simplifying and extending these interactions.\n",
      "\n",
      "Hereâ€™s how decorators relate to LangChain:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Logging and Debugging**\n",
      "LangChain provides built-in logging functionality, often implemented with decorators. You can use decorators to add logging to your custom chains or agents for tracking their execution.\n",
      "\n",
      "#### Example: Custom Logging Decorator in LangChain\n",
      "You can create a decorator to log the input and output of a chain:\n",
      "\n",
      "```python\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# Custom logging decorator\n",
      "def log_chain_execution(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(\"Chain execution started.\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(\"Chain execution completed.\")\n",
      "        print(f\"Result: {result}\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "# Define a simple chain\n",
      "template = \"What is the capital of {country}?\"\n",
      "prompt = PromptTemplate(template=template, input_variables=[\"country\"])\n",
      "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
      "\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# Decorate the chain's call method\n",
      "chain.__call__ = log_chain_execution(chain.__call__)\n",
      "\n",
      "# Execute the chain\n",
      "chain.run(country=\"France\")\n",
      "```\n",
      "\n",
      "#### Output:\n",
      "```\n",
      "Chain execution started.\n",
      "Chain execution completed.\n",
      "Result: Paris\n",
      "```\n",
      "\n",
      "- By using a decorator, you can attach logging functionality to the chain without modifying its internal logic.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Wrapping Tools or Agents**\n",
      "LangChain uses tools and agents to interact with external systems (e.g., databases, APIs). Decorators can be used to enhance these tools, such as by validating inputs, caching results, or logging their usage.\n",
      "\n",
      "#### Example: Validating Input for a Tool\n",
      "Suppose you are building a custom tool in LangChain (e.g., a weather API tool). You can use a decorator to validate the input before executing the tool logic.\n",
      "\n",
      "```python\n",
      "from langchain.tools import BaseTool\n",
      "\n",
      "# Custom validation decorator\n",
      "def validate_city(func):\n",
      "    def wrapper(city, *args, **kwargs):\n",
      "        if not city.isalpha():\n",
      "            raise ValueError(\"City name must contain only alphabetic characters.\")\n",
      "        return func(city, *args, **kwargs)\n",
      "    return wrapper\n",
      "\n",
      "# Define a custom tool\n",
      "class WeatherTool(BaseTool):\n",
      "    name = \"WeatherTool\"\n",
      "    description = \"Get weather information for a given city.\"\n",
      "\n",
      "    @validate_city\n",
      "    def _run(self, city: str):\n",
      "        # Mock weather API logic\n",
      "        return f\"The weather in {city} is sunny.\"\n",
      "\n",
      "    async def _arun(self, city: str):\n",
      "        return self._run(city)\n",
      "\n",
      "# Use the tool\n",
      "tool = WeatherTool()\n",
      "print(tool.run(\"London\"))  # Valid input\n",
      "print(tool.run(\"123\"))     # Invalid input (raises ValueError)\n",
      "```\n",
      "\n",
      "#### Output:\n",
      "```\n",
      "The weather in London is sunny.\n",
      "ValueError: City name must contain only alphabetic characters.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Enhancing Agents**\n",
      "Agents in LangChain rely on tools to execute queries and make decisions. Decorators can be used to *wrap* the agent's logic with additional functionality, such as monitoring its decision-making process or limiting the number of iterations.\n",
      "\n",
      "#### Example: Limiting Number of Agent Iterations\n",
      "You can use a decorator to enforce a maximum number of steps for an agent:\n",
      "\n",
      "```python\n",
      "from langchain.agents import AgentExecutor\n",
      "\n",
      "# Custom decorator to limit agent steps\n",
      "def limit_agent_steps(max_steps):\n",
      "    def decorator(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            agent_executor = args[0]  # The agent executor instance\n",
      "            if agent_executor.steps > max_steps:\n",
      "                raise RuntimeError(f\"Agent exceeded the maximum allowed steps: {max_steps}\")\n",
      "            return func(*args, **kwargs)\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "# Example agent class (mocked)\n",
      "class ExampleAgent(AgentExecutor):\n",
      "    steps = 0  # Track steps\n",
      "\n",
      "    @limit_agent_steps(max_steps=3)\n",
      "    def execute(self, query):\n",
      "        self.steps += 1\n",
      "        # Mocked logic for agent execution\n",
      "        return f\"Executing step {self.steps} for query: {query}\"\n",
      "\n",
      "# Use the agent\n",
      "agent = ExampleAgent()\n",
      "print(agent.execute(\"What is the weather in Paris?\"))\n",
      "print(agent.execute(\"Tell me more about it.\"))  # Runs until it exceeds the limit\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Integrating with Async Behavior**\n",
      "LangChain frequently uses async methods for interacting with tools or querying LLMs. Decorators can be used to handle asynchronous behavior, such as retrying failed requests or timing out long-running operations.\n",
      "\n",
      "#### Example: Timeout Decorator for Async Functions\n",
      "Hereâ€™s how you could decorate an async function in LangChain:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "\n",
      "# Timeout decorator for async functions\n",
      "def timeout(seconds):\n",
      "    def decorator(func):\n",
      "        async def wrapper(*args, **kwargs):\n",
      "            try:\n",
      "                return await asyncio.wait_for(func(*args, **kwargs), timeout=seconds)\n",
      "            except asyncio.TimeoutError:\n",
      "                raise RuntimeError(f\"Function '{func.__name__}' timed out after {seconds} seconds.\")\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "# Mock async tool\n",
      "class AsyncTool:\n",
      "    @timeout(3)  # Decorate the async method\n",
      "    async def _arun(self, query: str):\n",
      "        await asyncio.sleep(5)  # Simulate long-running task\n",
      "        return f\"Result for query: {query}\"\n",
      "\n",
      "tool = AsyncTool()\n",
      "\n",
      "# Run the async method\n",
      "try:\n",
      "    result = asyncio.run(tool._arun(\"What is LangChain?\"))\n",
      "    print(result)\n",
      "except RuntimeError as e:\n",
      "    print(e)\n",
      "```\n",
      "\n",
      "#### Output:\n",
      "```\n",
      "Function '_arun' timed out after 3 seconds.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**\n",
      "Decorators in LangChain allow developers to add functionality like logging, validation, caching, retries, and more to chains, tools, and agents without modifying their core logic. This makes the framework flexible and extensible for various use cases.\n",
      "\n",
      "If youâ€™d like to see more specific examples tied to LangChain workflows, let me know!\n",
      "\n",
      "\n",
      "Answer: Great question! In LangChain, decorators are heavily utilized to manage workflows, track execution, log events, and enhance the functionality of chains, agents, and tools.\n",
      "\n",
      "LangChain is a framework for building applications powered by large language models (LLMs), and it often involves complex interactions between various components (e.g., chains of prompts, tools, agents). Decorators can play a vital role in simplifying and extending these interactions.\n",
      "\n",
      "Hereâ€™s how decorators relate to LangChain:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Logging and Debugging**\n",
      "LangChain provides built-in logging functionality, often implemented with decorators. You can use decorators to add logging to your custom chains or agents for tracking their execution.\n",
      "\n",
      "#### Example: Custom Logging Decorator in LangChain\n",
      "You can create a decorator to log the input and output of a chain:\n",
      "\n",
      "```python\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# Custom logging decorator\n",
      "def log_chain_execution(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(\"Chain execution started.\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(\"Chain execution completed.\")\n",
      "        print(f\"Result: {result}\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "# Define a simple chain\n",
      "template = \"What is the capital of {country}?\"\n",
      "prompt = PromptTemplate(template=template, input_variables=[\"country\"])\n",
      "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
      "\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# Decorate the chain's call method\n",
      "chain.__call__ = log_chain_execution(chain.__call__)\n",
      "\n",
      "# Execute the chain\n",
      "chain.run(country=\"France\")\n",
      "```\n",
      "\n",
      "#### Output:\n",
      "```\n",
      "Chain execution started.\n",
      "Chain execution completed.\n",
      "Result: Paris\n",
      "```\n",
      "\n",
      "- By using a decorator, you can attach logging functionality to the chain without modifying its internal logic.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Wrapping Tools or Agents**\n",
      "LangChain uses tools and agents to interact with external systems (e.g., databases, APIs). Decorators can be used to enhance these tools, such as by validating inputs, caching results, or logging their usage.\n",
      "\n",
      "#### Example: Validating Input for a Tool\n",
      "Suppose you are building a custom tool in LangChain (e.g., a weather API tool). You can use a decorator to validate the input before executing the tool logic.\n",
      "\n",
      "```python\n",
      "from langchain.tools import BaseTool\n",
      "\n",
      "# Custom validation decorator\n",
      "def validate_city(func):\n",
      "    def wrapper(city, *args, **kwargs):\n",
      "        if not city.isalpha():\n",
      "            raise ValueError(\"City name must contain only alphabetic characters.\")\n",
      "        return func(city, *args, **kwargs)\n",
      "    return wrapper\n",
      "\n",
      "# Define a custom tool\n",
      "class WeatherTool(BaseTool):\n",
      "    name = \"WeatherTool\"\n",
      "    description = \"Get weather information for a given city.\"\n",
      "\n",
      "    @validate_city\n",
      "    def _run(self, city: str):\n",
      "        # Mock weather API logic\n",
      "        return f\"The weather in {city} is sunny.\"\n",
      "\n",
      "    async def _arun(self, city: str):\n",
      "        return self._run(city)\n",
      "\n",
      "# Use the tool\n",
      "tool = WeatherTool()\n",
      "print(tool.run(\"London\"))  # Valid input\n",
      "print(tool.run(\"123\"))     # Invalid input (raises ValueError)\n",
      "```\n",
      "\n",
      "#### Output:\n",
      "```\n",
      "The weather in London is sunny.\n",
      "ValueError: City name must contain only alphabetic characters.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Enhancing Agents**\n",
      "Agents in LangChain rely on tools to execute queries and make decisions. Decorators can be used to *wrap* the agent's logic with additional functionality, such as monitoring its decision-making process or limiting the number of iterations.\n",
      "\n",
      "#### Example: Limiting Number of Agent Iterations\n",
      "You can use a decorator to enforce a maximum number of steps for an agent:\n",
      "\n",
      "```python\n",
      "from langchain.agents import AgentExecutor\n",
      "\n",
      "# Custom decorator to limit agent steps\n",
      "def limit_agent_steps(max_steps):\n",
      "    def decorator(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            agent_executor = args[0]  # The agent executor instance\n",
      "            if agent_executor.steps > max_steps:\n",
      "                raise RuntimeError(f\"Agent exceeded the maximum allowed steps: {max_steps}\")\n",
      "            return func(*args, **kwargs)\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "# Example agent class (mocked)\n",
      "class ExampleAgent(AgentExecutor):\n",
      "    steps = 0  # Track steps\n",
      "\n",
      "    @limit_agent_steps(max_steps=3)\n",
      "    def execute(self, query):\n",
      "        self.steps += 1\n",
      "        # Mocked logic for agent execution\n",
      "        return f\"Executing step {self.steps} for query: {query}\"\n",
      "\n",
      "# Use the agent\n",
      "agent = ExampleAgent()\n",
      "print(agent.execute(\"What is the weather in Paris?\"))\n",
      "print(agent.execute(\"Tell me more about it.\"))  # Runs until it exceeds the limit\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Integrating with Async Behavior**\n",
      "LangChain frequently uses async methods for interacting with tools or querying LLMs. Decorators can be used to handle asynchronous behavior, such as retrying failed requests or timing out long-running operations.\n",
      "\n",
      "#### Example: Timeout Decorator for Async Functions\n",
      "Hereâ€™s how you could decorate an async function in LangChain:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "\n",
      "# Timeout decorator for async functions\n",
      "def timeout(seconds):\n",
      "    def decorator(func):\n",
      "        async def wrapper(*args, **kwargs):\n",
      "            try:\n",
      "                return await asyncio.wait_for(func(*args, **kwargs), timeout=seconds)\n",
      "            except asyncio.TimeoutError:\n",
      "                raise RuntimeError(f\"Function '{func.__name__}' timed out after {seconds} seconds.\")\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "# Mock async tool\n",
      "class AsyncTool:\n",
      "    @timeout(3)  # Decorate the async method\n",
      "    async def _arun(self, query: str):\n",
      "        await asyncio.sleep(5)  # Simulate long-running task\n",
      "        return f\"Result for query: {query}\"\n",
      "\n",
      "tool = AsyncTool()\n",
      "\n",
      "# Run the async method\n",
      "try:\n",
      "    result = asyncio.run(tool._arun(\"What is LangChain?\"))\n",
      "    print(result)\n",
      "except RuntimeError as e:\n",
      "    print(e)\n",
      "```\n",
      "\n",
      "#### Output:\n",
      "```\n",
      "Function '_arun' timed out after 3 seconds.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**\n",
      "Decorators in LangChain allow developers to add functionality like logging, validation, caching, retries, and more to chains, tools, and agents without modifying their core logic. This makes the framework flexible and extensible for various use cases.\n",
      "\n",
      "If youâ€™d like to see more specific examples tied to LangChain workflows, let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create memory\n",
    "qa_history = ChatMessageHistory()\n",
    "\n",
    "# Create a prompt with memory placeholder\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a knowledgeable AI assistant specialized in Python programming and LangChain. \"\n",
    "               \"Answer questions clearly and provide code examples when helpful.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "# Wrap with history\n",
    "qa_with_history = RunnableWithMessageHistory(\n",
    "    qa_chain,\n",
    "    lambda session_id: qa_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"What is a decorator in Python?\",\n",
    "    \"Can you show me an example?\",\n",
    "    \"How does this relate to LangChain?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {q}\")\n",
    "    print('='*60)\n",
    "    response = qa_with_history.invoke(\n",
    "        {\"question\": q},\n",
    "        config={\"configurable\": {\"session_id\": \"qa_session\"}}\n",
    "    )\n",
    "    print(f\"\\nAnswer: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f45ff",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Now that you've learned the fundamentals, here are some advanced topics to explore:\n",
    "\n",
    "1. **Document Loaders and Vector Stores**\n",
    "   - Load and process documents\n",
    "   - Create embeddings\n",
    "   - Build RAG (Retrieval Augmented Generation) systems\n",
    "\n",
    "2. **Advanced Agents**\n",
    "   - Custom agent types\n",
    "   - Tool integration (web search, APIs, databases)\n",
    "   - Multi-agent systems\n",
    "\n",
    "3. **LangSmith**\n",
    "   - Debugging and monitoring\n",
    "   - Prompt optimization\n",
    "   - Performance tracking\n",
    "\n",
    "4. **LangServe**\n",
    "   - Deploy LangChain applications as APIs\n",
    "   - Production-ready serving\n",
    "\n",
    "5. **Integration with Vector Databases**\n",
    "   - Pinecone, Weaviate, Chroma\n",
    "   - Semantic search\n",
    "   - Long-term memory\n",
    "\n",
    "### Resources\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "- [LangChain Discord Community](https://discord.gg/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! ðŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420abd19",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Tips\n",
    "\n",
    "Here are some key takeaways for working with LangChain:\n",
    "\n",
    "### 1. **Start Simple, Then Scale**\n",
    "- Begin with basic chains before implementing complex agents\n",
    "- Test each component individually before combining them\n",
    "\n",
    "### 2. **Memory Management**\n",
    "- Choose the right memory type for your use case:\n",
    "  - `ConversationBufferMemory`: Full history (good for short conversations)\n",
    "  - `ConversationBufferWindowMemory`: Limited history (better for long conversations)\n",
    "  - `ConversationSummaryMemory`: Summarized history (best for cost optimization)\n",
    "\n",
    "### 3. **Prompt Engineering**\n",
    "- Be specific in your prompts\n",
    "- Use system messages to set context and behavior\n",
    "- Include examples in your prompts (few-shot learning)\n",
    "\n",
    "### 4. **Error Handling**\n",
    "- Always handle potential errors in chains and agents\n",
    "- Use `handle_parsing_errors=True` for agents\n",
    "- Implement retry logic for API calls\n",
    "\n",
    "### 5. **Cost Optimization**\n",
    "- Monitor token usage\n",
    "- Use caching when appropriate\n",
    "- Choose the right model for your task (GPT-4 for complex, GPT-3.5 for simple tasks)\n",
    "\n",
    "### 6. **Testing**\n",
    "- Test prompts with various inputs\n",
    "- Validate agent tool selections\n",
    "- Monitor chain outputs for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edca1b",
   "metadata": {},
   "source": [
    "## 8. Practical Example: Building a Q&A System\n",
    "\n",
    "Let's build a complete question-answering system with memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48f819",
   "metadata": {},
   "source": [
    "## 7. Output Parsers\n",
    "\n",
    "Output parsers help structure the LLM's responses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520f84c",
   "metadata": {},
   "source": [
    "## 5. Memory - Maintaining Conversation Context\n",
    "\n",
    "Memory allows your application to remember previous interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "674e7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Unlocking the Power of Language Models with LangChain**\n",
      "\n",
      "In the rapidly evolving landscape of artificial intelligence, language models like OpenAIâ€™s GPT have become invaluable tools for tasks ranging from content creation to complex problem-solving. While these models are incredibly powerful, their full potential often lies untapped due to the complexity of integrating them into workflows or building dynamic, robust applications around them. Enter LangChainâ€”a transformative framework designed to maximize the utility of language models by enabling seamless integration, chaining capabilities, and advanced functionality.\n",
      "\n",
      "LangChain is a library that simplifies the process of building applications powered by language models. Whether you're developing a conversational AI, automating document processing, or generating code snippets, LangChain provides the tools to help you unlock new possibilities. Here are some key benefits of using LangChain:\n",
      "\n",
      "### 1. **Effortless Chaining for Complex Workflows**\n",
      "LangChain allows developers to chain multiple operations together, enabling the creation of sophisticated applications. For instance, you can implement a pipeline where a language model generates a summary, extracts key data, and queries additional informationâ€”all in a single workflow. This chaining capability helps you build modular, scalable solutions without reinventing the wheel.\n",
      "\n",
      "### 2. **Enhanced Context Management**\n",
      "One of the challenges with language models is maintaining context across interactions. LangChain provides tools to manage context effectively, ensuring your applications retain relevant information across multiple inputs and outputs. This is particularly useful for conversational agents, where continuity and coherence are critical.\n",
      "\n",
      "### 3. **Integration with External Data Sources**\n",
      "LangChain supports integration with external APIs, databases, and knowledge bases. This means your language model can access real-time data, historical records, or domain-specific information to generate more accurate and contextually rich outputs. For example, you can use LangChain to connect GPT with a financial database to answer complex investment-related questions.\n",
      "\n",
      "### 4. **Simplified Code and Rapid Prototyping**\n",
      "LangChain abstracts many of the complexities involved in working with language models, reducing the amount of boilerplate code required. This makes it easier for developers to create prototypes quickly and focus on refining their applications. Its intuitive design ensures that even those new to AI development can get started with minimal friction.\n",
      "\n",
      "### 5. **Customizable and Open-Source**\n",
      "LangChain is open-source, enabling developers to customize the framework to suit their specific needs. Whether you need to integrate proprietary APIs, tweak workflows, or optimize performance, LangChain offers the flexibility to adapt to your use case.\n",
      "\n",
      "### 6. **Support for Advanced Use Cases**\n",
      "LangChain isnâ€™t just for simple tasksâ€”it shines when it comes to complex applications like question answering, document retrieval, and knowledge generation. Its modular approach allows developers to experiment with different components, such as memory management, tools, and agents, to create solutions tailored to their requirements.\n",
      "\n",
      "### Conclusion\n",
      "LangChain is more than just a frameworkâ€”itâ€™s a game-changer for anyone looking to harness the full power of language models. By simplifying integration, enhancing functionality, and supporting complex workflows, LangChain empowers developers to build smarter, more efficient AI applications. Whether youâ€™re an experienced AI engineer or just starting your journey, LangChain is a tool worth exploring.\n",
      "\n",
      "Ready to unlock the potential of language models? Dive into LangChain today and transform your ideas into reality.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple chain using LCEL (LangChain Expression Language)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class technical writer.\"),\n",
    "    (\"user\", \"Write a short blog post about {topic}\")\n",
    "])\n",
    "\n",
    "# Chain: prompt -> llm -> output parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"topic\": \"the benefits of using LangChain\"})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
